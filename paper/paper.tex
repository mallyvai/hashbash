\documentclass{article}
\usepackage{amsmath}
\usepackage{cite}
\usepackage[dvips]{graphicx}
\usepackage{listings}
\bibliographystyle{plain}

\begin{document}
\title{Hash Bash: Genetic Algorithms for Hash Function Generation}
\author{Vaibhav Mallya}
\maketitle
\section{Abstract}


This paper explores the use of genetic algorithms for the generation of hash functions. We define a custom Turing-complete assembly language to represent our generated functions. We evaluate fitness primarily from collision-resistance. We can test uniform output distribution only intermittently due to the length and expense of the randomness tests. Speed is guaranteed by a hard maximum cycle limit. Our results are limited, but we believe the approach is promising.

\section{Introduction}
Hash functions are key building blocks of modern cryptography. They have applications in encryption algorithms, authentication protocols, and integrity verification schemes \cite{Cohen}. Today's most widely-used hash functions are inspired by Rivest's MD2 \cite{RivestMD2} and MD4, the designs of which are characterized by input chunking, substitution, and the repeated application of overlapping bitwise operations. Although there numerous other hashing schemes, they are either not as widely-used, or are not as effective, and therefore, not a focus of this paper. For the purposes of this paper, we will refer to all functions structurally related to the MDs and the SHAs as Rivest functions.

\paragraph{}
Although they are important to the soundness of many security schemes, most common modern hash functions, including SHA-1 and MD5, have either had serious attacks mounted against them or are in danger of being vulnerable in the near future.
MD5 in particular can now be broken in under a minute on a commodity machine. \cite{cryptoeprint:2005:067} As a result, an NIST competition has been initiated for the creation of a new cryptographically secure function \cite{NISTComp}. Unfortunately, hash function creation is an extremely difficult task, and vetting and verification takes a great deal of careful analysis.
Regardless, we show that it may be possible to approximate a good hash function through the use of the genetic algorithm.

\paragraph{}
The genetic algorithm is inspired by biological natural selection. John Holland of the University of Michigan pioneered its use in the 1970s \cite{Holland1973}.
However, it wasn't until the 1980s and 1990s that the computational capacity for practical implementations of the algorithm became readily available \cite{Goldberg}. The algorithm as we use it consists of a series of functions - a randomized generator, a fitness evaluator, and a breeder - that operate on chromosomes, which are strings formed from a genetic description alphabet. The algorithm proceeds as follows:
\begin{enumerate}
	\item Create a random starting generation of size S.
	\item Evaluate the fitness of every member of the current generation.
	\item Breed the fittest members to form the next generation.
	\item Insert new or old chromosomes into the next generation until it is of size S.
	\item Randomly mutate some member(s) of the generation.
	\item Go to step 2 if we're not done.
\end{enumerate}

With proper parameter selection and a sufficient amount of time, the genetic algorithm will tend to converge to some local optimum. Since we are combining parts of the best solutions to form new solutions, it seems reasonable that this should happen. Holland's Schemata Theorem provides a more rigid explanation as to how this occurs \cite{citeulike:1281572}.

\section{Language Description}
Since we are creating, breeding, and interpreting programs, we first need a language that meets the following criteria:

\begin{enumerate}
\item Readable enough to manually program and interpret.
\item Powerful enough to allow a wide range of hash functions to be efficiently and compactly represented.
\item Compact enough that there are few-to-none overlapping or redundant operators.
\item Flexible enough that syntax errors are impossible.
\item Ability to implement all Rivest functions.
\item Represented in a way that allows the breeding of two functions to yield a better function with high probability.
\end{enumerate}
\paragraph{}
An assembly language is an ideal candidate here. It's straightforward how machine code represents instructions, bitstrings are easy to manipulate, and disassembly is easy to interpret. Additionally, interpreters are easy to find or even write.
Although there are a number of special-purpose assembly languages that fulfill one or two of these criteria, we were unable to find any that fulfilled all of them. We created our own assembly language with the following characteristics:

\begin{itemize}
\item No registers - all operators work directly on memory.
	\begin{itemize}
          \item Somewhat impractical to implement in hardware, but this is not a design goal.
	\end{itemize}
	\item 2$^{15}$ words of memory, each 68 bits large. By providing more space, we hope to provide more effectiveness.
	\item 15 operators, each with four 16-bit operands.
	\begin{itemize}
		  \item Includes N/AND, X/OR, NOT, conditional sets, and conditional adds (to be used for branching)
          \item Every instruction does not utilize every operand.
          \item Leading bit of every operand indicates if the remaining 15 bits refer to an immediate or an address.
	\end{itemize}
	\item Program counter stored in a fixed location in memory, allowing the program to modify it (i.e. branching).
	\item A special operator \textbf{iterinput} that fetches the next input chunk and places it in a fixed memory location.
	\item Another fixed range of memory that is treated as an output buffer. When the program halts, the buffer is flushed and the bits in the output buffer at that time are viewed as the program's output.
\end{itemize}

\paragraph{}
Although the algorithm itself views this language in terms of its machine code, the human-readable disassembly is straightforward, since instructions take the form \textbf{[dest] [op] [op0] [op2]}. The machine code itself is just a series of binary strings. This makes programs trivial to:
	\begin{itemize}
	\item Generate: Given a template with gaps, randomly generate bitstrings of length 68 to fill the gaps.
	\item Breed: Concatenate the parent chromosomes.
		\begin{itemize}
			\item Hash functions concatenated should yield better hash functions, so logically, this is a good strategy.
			\item If our child function is too large, we remove words from the middle.
		\end{itemize}
	\item Mutate: AND some random bitstring(s) of length 68 with some random word(s).
		\begin{itemize}
		\item Allow a small number of additional words to be created with some small probability.
		\end{itemize}
	\end{itemize}

Finally, it's obvious that we can duplicate all Rivest functions in a straightforward way - we have access to all the bitwise operators used, as well as simple chunking and branching.

\section{Implementation Details and the Fitness Function}
The genetic algorithm and custom language simulator are implemented completely in Python, with the exception of the Diehard tests \cite{Diehard} which are written in C. There are four primary utility scripts: generate.py, fitness.py, simulate.py, and breed.py. Each has a corresponding parameters file for simple fine-tuning. The script that ties all of these together to compute current and successive generations is master.py, which has its own parameter file. Additionally, we add initial entropy to all hash functions through the use of a Munroe random number \cite{Munroe}. We initialize certain fixed ranges in memory to these numbers for potential use by a hash function.

\paragraph{}
We make extensive use of Psyco \cite{Psyco} and the processing module \cite{Processing}. Psyco improves individual script execution by utilizing a JIT compiler, while the processing module allows for quick and efficient parallelization of the algorithm itself.
The primary performance bottleneck, the fitness testing, is completely parallelized, with a pool of processes that greedily grab functions to simulate and evaluate. Breeding is less of a performance concern, but it is also parallelized due to the ease with which it is possible \cite{Mallya}. There are numerous additional opportunities for algorithmic parallelization that aren't utilized by our implementation.

\paragraph{}
The fitness function consists of two main parts. First, it attempts to filter out trivially bad functions by hashing the elements in [-10, 10]. If the function fails this trivial test by colliding at all, it is flagged. No further tests are conducted on it.
Next, it iterates over many random numbers, some of them very close to each other, to determine collision rates.
The fitness function writes the ratio (collisions $\div$ total inputs), and then terminates.
It is important to note that the Diehard \cite{Diehard} randomness and uniformity tests are not conducted programmatically due to their cost. Over 68 million bytes are required for a single hash function's test. The intended procedure for randomness testing is as follows:
\begin{enumerate}
	\item Manually create the named lockfile in the computational results directory to pause the algorithm.
	\item Initiate the randomness tests by hand.
	\item Manually append the results of the test to the fitness file of the relevant function. 
	\item The results will then be accounted for in the following generation's selection and breeding process.
\end{enumerate}

As a result of the randomness testing expense, we base our results primarily on the collision rates of the generated functions.

\section{Computational Results}
Unfortunately, the hash functions the algorithm currently generates are very weak, and would at best serve as CRCs or checksums of some sort. In all runs, we detected only minor differences between hash values for lexicographically adjacent inputs.
There were also a distressingly high number of overall functions ($\geq$ 15\%) that produced only constants. Upon reexamination, we found several design decisions that seem to have contributed to the exceptionally poor quality of generated functions.

\subsection{Too many conditional operators}
Out of 16 instructions, there are 3 conditional sets and 3 conditional adds. These were intended to be used for branching, so one of the generators actually creates branches with an overly high probability. This obviously excludes bitwise operators that are supposed to do the bulk of the computation from appearing as much as would be ideal.

\subsection{Poor generator function}
There are in fact four structured generator functions provided, but only one of them is actually used most of the time, since the others never produce anything but constants. The single usable generator is itself a manually-created and very trivial hash function. It simply returns some number of bits from the first block of the input. For small inputs, it is a kind of perfect hash function, and obviously produces collisions for longer inputs. Originally, we had intended to create a plethora of generators and randomly choose one when generating a new cell, but this was scrapped due to time constraints.

\subsection{Too-large word size; too many words}
These are related problems; the 68-bit word size is a by-product of wanting to address 2$^{15}$ words of memory with three operands. In retrospect, this was a terrible decision, since there is never a need to utilize anything beyond the first 2000 words or so for the program and all temporary variables it could reasonably use. 2000 itself is a very generous upper bound. By providing so much space, we provide the algorithm with a lot of room to do a lot of nothing.

\subsection{Turing-completeness}
Turing completeness grants the power to solve more difficult problems. But in the context of the genetic algorithm, it also leaves significant opportunities to solve even simple problems incorrectly. More power requires more responsibility, and the genetic algorithm is not responsible at all. As implemented in most languages, the Rivest functions' operators include looping and conditionals. However, when we look at diagrams of the functions themselves, we notice the information stream always ``flows down''. There is no backwards flow \cite{MD5Diagram}. We can therefore implement Rivest functions with a conditional relative branch whose offset is always positive. There is no need to have a negative branch value. There will be an obscene amount of code repetition with this design, since we'll have to duplicate the code for every round over and over, but it's a computationally valid way of approaching the problem. By eliminating language features that allow Turing completeness, we can simply fitness testing and generation without sacrificing functionality of the hash function. Overall, it should eliminate unfit members much faster. To take advantage of this, we would have to make the following changes to our language:
\begin{enumerate}
\item Remove the program counter from the unified memory, so it is not open to arbitrary modification.
\item Add a new branch-on-equality operator with three operands.
\item Make this operator increment the program counter by $|\textbf{op3}|$ if (\textbf{op1} == \textbf{op2})
\end{enumerate}

We feel the question of not needing a Turing-complete language to implement a Rivest function is an interesting one that merits further analysis.

\section{Further Thoughts and Conclusions}
Despite all of this, we believe this approach holds promise, and given more time, we feel we can produce more substantial and interesting results. The justification for our optimism goes back to our generator function. As mentioned, it's a trivial hash function that just moves some subset of the input buffer into the output buffer. Indeed, on first glance, it would seem like a poor candidate for an initial function. However, we conducted several runs ($\approx$ 50 generations) with the initial population consisting entirely of this 5-line function. At the end of each of these runs, we noticed that there were two or three long functions ($>$300 lines) whose exact hash outputs varied slightly from their initial forms. The variation wasn't significant, but the fact that phenotypic variation existed at all is a hopeful sign. It stands to reason that if provided additional trivial hash functions that collectively exercised the full power of the assembly language, the generator would help the algorithm to produce substantially more interesting input.

\paragraph{}
Also of note is the fact that the resources available for the purposes of this paper were limited. We only had part-time access to a single dual-core machine that was frequently used for other tasks, and only one (very time-strapped) developer.  With the injection of additional human resources, the algorithm could be adapted to run on a cluster, the cloud, etc with the modifications as outlined above implemented. Although the runs conducted with this iteration produced limited results, fixing the deficiencies as outlined above would definitely allow for more rapid convergence on interesting behavior.

\newpage
\bibliography{paper_cite}
\end{document}