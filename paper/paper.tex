\documentclass{article}
\usepackage{amsmath}
\usepackage{cite}
\usepackage[dvips]{graphicx}
\usepackage{listings}
\bibliographystyle{plain}

\begin{document}
\title{Hash Bash: Genetic Algorithms for Hash Function Generation}
\author{Vaibhav Mallya}
\maketitle
\section{Abstract}


This paper explores the use of genetic algorithms for the generation of hash functions. We define a custom Turing-complete assembly language to represent our generated functions. We evaluate fitness primarily from collision-resistance. We can test uniform output distribution only intermittently due to the length and expense of the randomness tests. Speed is guaranteed by a hard maximum cycle limit. Our results are limited, but we believe the approach is promising.

\section{Introduction}
Hash functions are key building blocks of modern cryptography, having applications in encryption algorithms, authentication protocols, and integrity verification schemes \cite{Cohen}. Today's most widely-used hash functions are inspired by Rivest's MD2 \cite{RivestMD2} and MD4, the designs of which are characterized by input chunking, substitution, and the repeated application of overlapping bitwise operations. Although there numerous other hashing schemes, they are not as widely-used, and therefore, not a focus of this paper.

\paragraph{}
Although they are important to the soundness of many security schemes, most modern commonly-used hash functions, including SHA-1 and MD5 (hereafter referred to as Rivest functions), have either had serious attacks mounted against them or are in danger of being vulnerable in the near future.
MD5 in particular can now be broken in under a minute on a commodity machine. \cite{cryptoeprint:2005:067} As a result, an NIST competition has been initiated for the creation of a new cryptopgraphically secure function \cite{NISTComp}. Unfortunately, hash function creation is an extremely difficult task, and vetting and verification takes a great deal of careful analysis.
However, we show that it may be possible to approximate a good hash function through the use of the genetic algorithm.

\paragraph{}
The genetic algorithm is inspired by biological natural selection. John Holland of the University of Michigan pioneered its use in the 1970s \cite{Holland1973}.
However, it wasn't until the 1980s and 1990s that the computational capacity for practical implementations of the algorithm became readily available \cite{Goldberg}. The algorithm as we use it consists of a series of functions - a randomized generator, a fitness evaluator, and a breeder - that operate on chromomsomes, which are strings formed from a genetic description alphabet. The algorithm proceeds as follows:
\begin{enumerate}
	\item Create a random starting population of size S.
	\item Evaluate the fitness of every member of the current generation.
	\item Breed the fittest members to form the next generation.
	\item Add random or old members until we have the next generation is once more of size S.
	\item Randomly mutate some member(s) of the generation.
	\item Go to step 2 if we're not done.
\end{enumerate}

With proper parameter selection and a sufficient amount of time, the genetic algorithm will tend to converge to some local optimum. Since we are combining parts of the best solutions to form new solutions, it seems reasonable that this should happen. Holland's Schemata Theorem provides a more rigid explanation as to how this occurs \cite{citeulike:1281572}.

\section{Language Description}
Since we are creating, breeding, and interpreting programs, we first need a language that meets the following criteria:

\begin{enumerate}
\item Readable enough to manually program and interpret.
\item Powerful enough to allow a wide range of hash functions to be efficiently and compactly represented.
\item Compact enough that there are few-to-none overlapping or redundant operators.
\item Flexible enough that syntax errors are impossible.
\item Ability to implement all Rivest functions
\item Represented in a way that allows the breeding of two functions to yield a better function with high probability
\end{enumerate}
\paragraph{}
An assembly language is an ideal candidate here. It's straightforward how machine code represents instructions, bitstrings are easy to manipulate, and dissassembly is easy to interpret. Additionally, interpreters are easy to find or even write.
Although there are a number of special-purpose assembly languages that fulfill one or two of these criteria, we were unable to find any that fulfilled all of them. We created our own assembly language with the following characteristics:

\begin{itemize}
\item No registers - all operators work directly on memory.
	\begin{itemize}
          \item Somewhat impractical to implement in hardware, but this is not a design goal.
	\end{itemize}
	\item 2$^{15}$ words of memory, each 68 bits large.
	\item 15 operators, each allowing four 16-bit operands.
	\begin{itemize}
		  \item Includes N/AND, X/OR, NOT, conditional sets, and conditional adds
          \item Every instruction does not utilize every operand.
          \item Leading bit of every operand indicates if the remaining 15 bits refer to an immediate or an address.
	\end{itemize}
	\item Program counter stored in a fixed location in memory, allowing the program to modify it (i.e. branching).
	\item A special operator \textbf{iterinput} that fetches the next input chunk and places it in a fixed memory location.
	\item This fixed section of memory is treated as an output buffer.
\end{itemize}

\paragraph{}
Although the algorithm itself views this language in terms of its machine code, the human-readable disassembly is straightforward, since instructions take the form \textbf{[dest] [op] [op0] [op2]}. The machine code itself is just a series of binary strings. This makes programs trivial to:
	\begin{itemize}
	\item Generate: Given a template with "gaps", randomly generate bitstrings of length 68 to fill the gaps.
	\item Breed: Concatenate the parent chromosomes.
		\begin{itemize}
			\item Hash functions concatenated should yield better hash functions, so logically, this is a good strategy.
			\item If our child function is too large, we remove words from the middle.
		\end{itemize}
	\item Mutate: AND some random bitstring(s) of length 68 with some random word(s). Also allow an additional word to be created with some small probability.
	\end{itemize}

Finally, it's obvious that we can duplicate all Rivest functions in a straightforward way - we have access to all the bitwise operators used, as well as simple chunking and branching.

\section{Implementation Details and the Fitness Function}
The genetic algorithm and custom language simulator are implemented completely in Python, with the exception of the Diehard tests \cite{Diehard} which are written in C. There are four primary utility scripts: generate.py, fitness.py, simulate.py, breed.py. Each has a corresponding parameters file for simple fine-tuning. The script that ties all of these together to compute current and successive generations is master.py, which has its own parameters file.

\paragraph{}
We make extensive use of Psyco \cite{Psyco} and the processing module \cite{Processing}. Psyco improves individual script execution by utilizing a JIT compiler, while the processing module allows for quick and efficient parallelization of the algorithm itself.
The primary performance bottleneck, the fitness testing, is completely parallelized, with a pool of processes that greedily grab functions to simulate and evaluate. Breeding is less of a performance concern, but it is also parallelized due to the ease with which it is possible. There are numerous additional opportunities for algorithmic parallelization that aren't utilized by our implementation.

\paragraph{}
The fitness function consists of two main parts. First, it attempts to filter out trivially bad functions by hashing the elements in [-10, 10]. If the function fails this trivial test by colliding at all, it is flagged. No further tests are conducted on it.
Next, it iterates over many random numbers, some of them very close to each other, to determine collision rates.
The fitness function notes the ratio collisions $\div$ total inputs, and then terminates.
It is important to note that randomness and uniformity tests are not conducted programmatically due to their cost. Over 68 million bytes are required for a single hash function's test. The intended procedure for randomness testing is as follows: 
\begin{enumerate}
	\item Manually create the named lockfile in the computational results directory to pause the algorithm.
	\item Initiate the randomness tests by hand.
	\item Manually append the results of the test to the fitness file of the relevant function. 
	\item The results will then be accounted for in the following generation's selection and breeding process.
\end{enumerate}

As a result of the expens, we base our results primarily on the collision rates of the generated functions.

\section{Computational Results}
Unfortunately, the hash functions the algorithm currently generates are very weak, and would at best serve as CRCs or checksums of some sort. In all runs, we detected only minor differences between hash values for lexicographically adjacent inputs.
There were also a distressingly high number of overall functions ($\geq$ 15\%) that produced only constants. Upon reexamination, we found several design decisions that seem to have contributed to poor quality.

\subsection{1. Too many conditional operators}
Out of 16 instructions, there are 3 conditional sets and 3 conditional adds. These were intended to be used for branching, so one of the generators actually creates branchs with an overly high probability. This obviously excludes bitwise operators that are supposed to do the bulk of the computation from appearing as much as would be ideal.

\subsection{2. Poor generator function}
There are in fact four structured generator functions provided, but only one of them is actually used most of the time, since the others never produce anything but constants. The single usable generator is itself a manually-created and very trivial hash function. It simply returns some number of bits from the first block of the input. For small inputs, it is a kind of perfect hash function (http://burtleburtle.net/bob/hash/perfect.html), and obviously produces collisions for longer inputs. Originally, we had intended to create a plethora of generators and randomly choose one when generating a new cell, but this was scrapped due to general generator ineffectiveness.

\subsection{3. Too-large word size; too many words}
These are related problems; the 68-bit word size is a by-product of wanting to address 2$^15$ words of memory with three operands. In retrospect, this was a terrible decision, since there is never a need to utilize anything beyond the first 2000 words or so for the program and all temporary variables. 2000 itself is a very generous upper bound.

\subsection{4. Turing-completeness}
Turing completeness grants the power to solve more difficult problems. But in the context of the genetic algorithm, it also leaves significant opportunities to solve even simple problems incorrectly. More power requires more responsibility, and the genetic algorithm is not responsible at all. As implemented in most languages, the Rivest functions' operators include looping and conditionals. However, when we look at diagrams of the functions themselves, we notice the information stream always "flows down". There is no flow backwards \cite{MD5Diagram}. We can therefore implement Rivest functions with a conditional relative branch whose offset is always positive. There is no need to have a negative branch value. There will be an obscene amount of code repetition with this design since we'll have to duplicate the code for every round over and over, but it's a computationally valid way of approaching the problem. By eliminating language features that allow Turing completeness, we can simply fitness testing and generation without sacrifing functionality of the hash function. To take advantage of this fact, we have to make the following changes to our language:
\begin{enumerate}
\list Remove the program counter from the unified memory, so it is not open to arbitrary modification.
\list Add a new branch-on-equality operator with three operands.
\list It should increment the program counter by \textbf{|op3|} if \textbf{op1} == \textbf{op2}
\end{enumerate}

We feel this is an interesting point that merits further analysis.

\section{Further Thoughts and Conclusion}
Despite all of this, we believe this approach holds promise, and given more time, we feel we can produce more substantial and interesting results. The justification for our optimism goes back to our generator function. As mentioned, it's a trivial hash function that just moves some subset of the input buffer into the output buffer. Indeed, on first glance, it would seem like a poor candidate for an initial function. However, we conducted several runs ($\approx$ 50 generations) with the initial population consisting entirely of this 5-line function. At the end of each of these runs, we noticed that there were two or three long functions ($>$300 lines) whose exact hash outputs varied slightly from their initial forms. The variation wasn't significant, but the fact that phenotypic variation existed at all is a hopeful sign. It stands to reason that if provided additional trivial hash functions that collectively exercised the full power of the assembly language, the generator would help the algorithm to produce substantially more interesting input.

Also of importance the resources available for the purposes of this paper were limited. We only had part-time access to a single dual-core machine that was frequently used for other tasks, and only one (time-strapped) developer.  With the injection of additional human resources, the algorithm could be adapted to run on a cluster, the cloud, etc. It would definitely allow for more rapid convergence on interesting behavior. Since the algorithm is embarassingly parallel, we feel this would be an extremely feasible conversion.
\bibliography{paper_cite}
\end{document}